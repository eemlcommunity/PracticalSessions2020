{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet18_cifar10_SE_solution.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGuivbg9OJ-w",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Add [Squeeze-and-Excitation (SE)](https://arxiv.org/pdf/1709.01507.pdf) blocks to our Resnet18 baseline\n",
        "\n",
        "The Squeeze-and-Excitation blocks (figures from original paper [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)) \n",
        "![SE block](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/assets/SE.png?raw=true)\n",
        "implement a simple form of *self-attention*. A self-attention module, in general, receives a number of inputs and produces the same number of outputs. The operations applied inside the block allow the inputs to interact with each other (\"self\") and recalibrate each other, based on which are deemed more important for that particular sample. \n",
        "\n",
        "The goal of SE block is to apply self-attention at the level of feature channels to allow them to recalibrate using more global information (as opposed to the local information available to conv units).    \n",
        "\n",
        "The Squeeze-and-excitation block has two steps:\n",
        "* *squeeze*: Given an input feature block, extract a global descriptor, one value per channel; a simple global descriptor can be obtained through spatial average global pooling\n",
        "* *excite*: Based on the global descriptor, learn a weight vector (using an MLP) to be applied as a gating mechanism on the original features. The output non-linearity of the block is a `sigmoid` (and not e.g. a softmax) to allow multiple feature channels to be emphasised. These weights are then applied (through broadcasting) over the original features, emphasising the features that are more important for those inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKodranAoYYz",
        "colab_type": "text"
      },
      "source": [
        "### SE-Resnet module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0QccuQGbGgc",
        "colab_type": "text"
      },
      "source": [
        "To apply the SE block to a Resnet architecture, we simply insert the SE block within each resnet block before the element-wise addition between the shortcut and the residual connections\n",
        "\n",
        "![alt text](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/assets/SEresnet.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUhxehxYHuuY",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "from typing import Iterable, Mapping, Tuple, Generator, Optional, Sequence, Text, Union, List\n",
        "\n",
        "# We will use haiku on top of jax; it is not included by default, so let's install it  \n",
        "!pip install -q dm-haiku\n",
        "import haiku as hk\n",
        "\n",
        "import jax\n",
        "from jax.experimental import optix  # package for optimizer\n",
        "import jax.numpy as jnp  # equivalent of numpy on GPU and TPU\n",
        "import numpy as np  # original numpy\n",
        "\n",
        "!pip install -q dm-tree\n",
        "import tree\n",
        "import enum\n",
        "import time\n",
        "\n",
        "# Dataset libraries\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Plotting library.\n",
        "from matplotlib import pyplot as plt\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# Define useful types.\n",
        "OptState = Tuple[optix.TraceState, optix.ScaleByScheduleState, optix.ScaleState]\n",
        "Scalars = Mapping[str, jnp.ndarray]\n",
        "Batch = Mapping[Text, np.ndarray]\n",
        "ClassNames = Mapping[List, str]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah2fsQ2DI1IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset constants for cifar10 dataset, the \"MNIST of real images\":\n",
        "# it contains low-res natural images (32x32x3) belonging to 10 classes.\n",
        "dataset_name = 'cifar10'\n",
        "class_cifar10 = [u'airplane', u'automobile', u'bird', u'cat', u'deer', u'dog', u'frog', u'horse', u'ship', u'truck'] \n",
        "train_split = 'train'\n",
        "eval_split = 'test'\n",
        "num_examples = {train_split: 50000,\n",
        "                eval_split: 10000}\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maQt-JG5HChv",
        "colab_type": "text"
      },
      "source": [
        "### Hyper-parameters for training and optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD0PWPguI_Xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size = 128 #@param\n",
        "eval_batch_size = 100  #@param\n",
        "model_bn_decay = 0.9  #@param\n",
        "train_weight_decay = 1e-4  #@param\n",
        "optimizer_momentum = 0.9  #@param\n",
        "optimizer_use_nesterov = True  #@param\n",
        "train_eval_every = 1000  #@param\n",
        "train_init_random_seed = 42  #@param\n",
        "train_log_every = 100  #@param\n",
        "num_train_steps = 400e3  #@param\n",
        "num_eval_steps = (num_examples[eval_split]) // eval_batch_size\n",
        "\n",
        "# Reduction ratio for the SE blocks\n",
        "reduction_ratio = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPD4on8RD2Z3",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Dataset loading and preprocessing\n",
        "# We use tensorflow readers; JAX does not have support for input data reading\n",
        "# and pre-processing.\n",
        "def load(split: str,\n",
        "         *,\n",
        "         is_training: bool,\n",
        "         batch_size: int) -> Generator[Batch, None, None]:\n",
        "  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
        "  ds = tfds.load('cifar10', split=split).cache().repeat()\n",
        "  \n",
        "  if is_training:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "\n",
        "  # Define the preprocessing for each train and test image\n",
        "  def preprocess(example):\n",
        "    image = _preprocess_image(example['image'], is_training)\n",
        "    return {'image': image, 'label': example['label']}\n",
        "\n",
        "  # Apply the preprocessing function to all samples in a batch using `map`\n",
        "  ds = ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Get samples grouped in mini-batches to train using SGD\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return tfds.as_numpy(ds)  # return numpy array\n",
        "\n",
        "def _preprocess_image(\n",
        "    image: tf.Tensor,\n",
        "    is_training: bool,\n",
        ") -> tf.Tensor:\n",
        "  \"\"\"Returns processed and resized image.\"\"\"\n",
        "  # Images are stored as uint8; we convert to float for further processing.\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  # Normalise pixel values between -1 and 1: original images are in [0, 255].\n",
        "  # We normalise to [-1, 1] to have 0 mean and unit variance in the inputs,\n",
        "  # as it makes the training more stable. Note that we do this normalisation \n",
        "  # over the activations of all the layers in the network by using batch \n",
        "  # normalisation layers.\n",
        "  image = 2 * (image / 255.0) - 1.0\n",
        "\n",
        "  # During training, we use data augmentation (left-right flips, random crops).\n",
        "  # In this way, we are effectively increasing the size of the training dataset,\n",
        "  # leading to improved generalisation.\n",
        "  if is_training:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    # Pad images by reflecting the boundaries and randomly sample a 32x32 patch.\n",
        "    image = tf.pad(image, [[4, 4], [4, 4], [0, 0]], mode='REFLECT')\n",
        "    image = tf.image.random_crop(image, size=(32, 32, 3))\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HtjHZ3FMeoe",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Function to display images\n",
        "MAX_IMAGES = 8\n",
        "def gallery(images: np.ndarray,\n",
        "            label: np.ndarray,\n",
        "            class_names: ClassNames=class_cifar10,\n",
        "            title: str='Input images'):  \n",
        "  \"\"\"Display a batch of images.\"\"\"\n",
        "  num_frames, h, w, num_channels = images.shape\n",
        "  num_frames = min(num_frames, MAX_IMAGES)\n",
        "  ff, axes = plt.subplots(1, num_frames,\n",
        "                          figsize=(32, 32),\n",
        "                          subplot_kw={'xticks': [], 'yticks': []})\n",
        "  if images.min() < 0:\n",
        "    images = (images + 1.) / 2.\n",
        "  for i in range(0, num_frames):\n",
        "    if num_channels == 3:\n",
        "      axes[i].imshow(np.squeeze(images[i]))\n",
        "    else:\n",
        "      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "    axes[i].set_title(class_names[label[i]], fontsize=28)\n",
        "    plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "    plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "  ff.subplots_adjust(wspace=0.1)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXT0-1Wfb-xP",
        "colab_type": "text"
      },
      "source": [
        "### SE block (coding exercise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M35NelBRcFXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SEBlock(hk.Module):\n",
        "  \"\"\"Squeeze and Excitation block.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      channels: Sequence[int],\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    check_length(2, channels, \"channels\")\n",
        "    self.fc1 = hk.Linear(channels[0])\n",
        "    self.fc2 = hk.Linear(channels[1])\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "    # Apply the squeeze step using spatial average global pooling\n",
        "    out = jnp.mean(inputs, axis=[1, 2])\n",
        "    # Excite step: linear layer, ReLU, linear layer, sigmoid\n",
        "    out = jax.nn.relu(self.fc1(out))\n",
        "    out = jax.nn.sigmoid(self.fc2(out))\n",
        "    # Scale the input features with the obtained self-attention weights\n",
        "    out = jnp.expand_dims(out, axis=(1, 2))\n",
        "    out = inputs * out\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j5qHouauJeS",
        "colab_type": "text"
      },
      "source": [
        "### Create a resnet block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbSRNkW-uFmL",
        "colab_type": "text"
      },
      "source": [
        "In a typical sequential model (no branching), the network as a whole is optimised to find the mapping between inputs and correct labels. In residual networks, each layer can learn an additive residual representation wrt to the representation already computed up to the previous layer, making the optimisation easier.\n",
        "\n",
        "As opposed to [resnet-v1](https://arxiv.org/pdf/1512.03385.pdf) blocks (left), [resnet-v2](https://arxiv.org/pdf/1603.05027.pdf) blocks (right) use pre-activation modules, i.e. the batch normalisation (`BN`) and relu (`ReLU`) nonlinearity are applied within the resnet block, before the convolutional layer (`weight`). This allows the model to learn identity mappings over the shortcuts throughout the network, improving further the backpropagation of gradients.   \n",
        "\n",
        "<img src=\"https://github.com/eemlcommunity/PracticalSessions2020/blob/master/assets/v1v2.png?raw=true\" alt=\"resnet blocks\" style=\"width: 80px;\"/>\n",
        "\n",
        "Figure from original [resnet-v2 paper](https://arxiv.org/pdf/1603.05027.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZa53uq8CEx",
        "colab_type": "text"
      },
      "source": [
        "*Bottleneck blocks*: To reduce the number of parameters and memory footprint without sacrificing expressivity, bottleneck blocks can be applied. Instead of using 2 conv layers (`weight` in the figure above) with 3x3 filters, empirically it is shown that projecting in a lower dimensional space (using 1x1 conv layers), applying 3x3 convolutions, and then reprojecting back into the original dimension space, does not affect accuracy.    \n",
        "\n",
        "*1x1 conv shortcuts*: when the input and the output of a resnet block have different numbers of channels, 1x1 convolutional layers are used on the shortcut to project the representation to the desired output feature dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkrrM05HdwGf",
        "colab_type": "text"
      },
      "source": [
        "## Resnet block with self-attention (coding exercise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enwxoMEatMQM",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def check_length(length, value, name):\n",
        "  if len(value) != length:\n",
        "    raise ValueError(f\"`{name}` must be of length 4 not {len(value)}\")\n",
        "\n",
        "class BlockV2(hk.Module):\n",
        "  \"\"\"ResNet V2 block with optional bottleneck.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      channels: int,\n",
        "      stride: Union[int, Sequence[int]],\n",
        "      use_projection: bool,\n",
        "      bn_config: Mapping[str, float],\n",
        "      bottleneck: bool,\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self.use_projection = use_projection\n",
        "\n",
        "    # Define batch norm parameters: the batch_norm layer normalises the inputs \n",
        "    # to have zero mean and unit variance. To not affect the expressivity\n",
        "    # of the network, e.g. in cases where it would be better for the activations\n",
        "    # to not be 0-centred or to have larger variance, batch_norm can optionally\n",
        "    # learn a scale and an offset parameters. \n",
        "    bn_config = dict(bn_config)\n",
        "    bn_config.setdefault(\"create_scale\", True)\n",
        "    bn_config.setdefault(\"create_offset\", True)\n",
        "\n",
        "    # See comment above about 1x1 conv shortcut \n",
        "    if self.use_projection:\n",
        "      self.proj_conv = hk.Conv2D(\n",
        "          output_channels=channels,\n",
        "          kernel_shape=1,\n",
        "          stride=stride,\n",
        "          with_bias=False,\n",
        "          padding=\"SAME\",\n",
        "          name=\"shortcut_conv\")\n",
        "\n",
        "    # If we use bottleneck blocks (see comment above), inside the resnet block \n",
        "    # we first project the activations into a lower dimensional space, \n",
        "    # which has number of channels divided by `channel_div` compared to the \n",
        "    # desired number of channels in the output.\n",
        "    channel_div = 4 if bottleneck else 1\n",
        "    conv_0 = hk.Conv2D(\n",
        "        output_channels=channels // channel_div,\n",
        "        kernel_shape=1 if bottleneck else 3,\n",
        "        stride=1,\n",
        "        with_bias=False,\n",
        "        padding=\"SAME\",\n",
        "        name=\"conv_0\")\n",
        "\n",
        "    bn_0 = hk.BatchNorm(name=\"batchnorm_0\", **bn_config)\n",
        "    # Then we apply the 3x3 conv layer\n",
        "    conv_1 = hk.Conv2D(\n",
        "        output_channels=channels // channel_div,\n",
        "        kernel_shape=3,\n",
        "        stride=stride,\n",
        "        with_bias=False,\n",
        "        padding=\"SAME\",\n",
        "        name=\"conv_1\")\n",
        "\n",
        "    bn_1 = hk.BatchNorm(name=\"batchnorm_1\", **bn_config)\n",
        "    layers = ((conv_0, bn_0), (conv_1, bn_1))\n",
        "\n",
        "    # When using bottleneck, we have also a 3rd 1x1 convolutional layer\n",
        "    # within the resnet block (see comment above about bottleneck blocks)\n",
        "    if bottleneck:\n",
        "      conv_2 = hk.Conv2D(\n",
        "          output_channels=channels,\n",
        "          kernel_shape=1,\n",
        "          stride=1,\n",
        "          with_bias=False,\n",
        "          padding=\"SAME\",\n",
        "          name=\"conv_2\")\n",
        "\n",
        "      bn_2 = hk.BatchNorm(name=\"batchnorm_2\", **bn_config)\n",
        "      layers = layers + ((conv_2, bn_2),)\n",
        "\n",
        "    self.layers = layers\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "    # Define the self-attention block\n",
        "    self.se_block = SEBlock([channels // reduction_ratio, channels])\n",
        "\n",
        "  def __call__(self, inputs, is_training, test_local_stats):\n",
        "    x = shortcut = inputs\n",
        "    for i, (conv_i, bn_i) in enumerate(self.layers):\n",
        "      # Apply pre-activation: batch_norm + relu\n",
        "      x = bn_i(x, is_training, test_local_stats)\n",
        "      x = jax.nn.relu(x)\n",
        "      # If using 1x1 conv projection on the shortcut, apply proj_conv once \n",
        "      if i == 0 and self.use_projection:\n",
        "        shortcut = self.proj_conv(x)\n",
        "      # Apply convolution\n",
        "      x = conv_i(x)\n",
        "\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "    # Apply the SE block before the element-wise addition between the shortcut\n",
        "    # and the residual\n",
        "    x = self.se_block(x)\n",
        "    \n",
        "    return x + shortcut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu3_qgA-7N0G",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Stack resnet blocks\n",
        "class BlockGroup(hk.Module):\n",
        "  \"\"\"Group of blocks for ResNet implementation.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      channels: int,\n",
        "      num_blocks: int,\n",
        "      stride: Union[int, Sequence[int]],\n",
        "      bn_config: Mapping[str, float],\n",
        "      bottleneck: bool,\n",
        "      use_projection: bool,\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self.blocks = []\n",
        "    for i in range(num_blocks):\n",
        "      self.blocks.append(\n",
        "          BlockV2(channels=channels,\n",
        "                  stride=(1 if i else stride),\n",
        "                  use_projection=(i == 0 and use_projection),\n",
        "                  bottleneck=bottleneck,\n",
        "                  bn_config=bn_config,\n",
        "                  name=\"block_%d\" % (i)))\n",
        "\n",
        "  def __call__(self, inputs, is_training, test_local_stats):\n",
        "    out = inputs\n",
        "    for block in self.blocks:\n",
        "      out = block(out, is_training, test_local_stats)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef_YOIYiDy7H",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define a generic resnet architecture\n",
        "# Note: This class is generic, it can be used to instantiate any Resnet \n",
        "# model, e.g. Resnet-50, Resnet-101, etc. by substituting the correct block\n",
        "# parameters \n",
        "class ResNet(hk.Module):\n",
        "  \"\"\"ResNet model.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      blocks_per_group: Sequence[int],\n",
        "      num_classes: int,\n",
        "      bn_config: Optional[Mapping[str, float]] = None,\n",
        "      bottleneck: bool = True,\n",
        "      channels_per_group: Sequence[int] = (256, 512, 1024, 2048),\n",
        "      use_projection: Sequence[bool] = (True, True, True, True),\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    \"\"\"Constructs a ResNet model.\n",
        "    Args:\n",
        "      blocks_per_group: A sequence of length 4 that indicates the number of\n",
        "        blocks created in each group.\n",
        "      num_classes: The number of classes to classify the inputs into.\n",
        "      bn_config: A dictionary of two elements, `decay_rate` and `eps` to be\n",
        "        passed on to the `BatchNorm` layers. By default the `decay_rate` is\n",
        "        `0.9` and `eps` is `1e-5`.\n",
        "      bottleneck: Whether the block should bottleneck or not. Defaults to True.\n",
        "      channels_per_group: A sequence of length 4 that indicates the number\n",
        "        of channels used for each block in each group.\n",
        "      use_projection: A sequence of length 4 that indicates whether each\n",
        "        residual block should use projection.\n",
        "      name: Name of the module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    bn_config = dict(bn_config or {})\n",
        "    bn_config.setdefault(\"decay_rate\", 0.9)\n",
        "    bn_config.setdefault(\"eps\", 1e-5)\n",
        "    bn_config.setdefault(\"create_scale\", True)\n",
        "    bn_config.setdefault(\"create_offset\", True)\n",
        "\n",
        "    # Number of blocks in each group for ResNet.\n",
        "    check_length(4, blocks_per_group, \"blocks_per_group\")\n",
        "    check_length(4, channels_per_group, \"channels_per_group\")\n",
        "\n",
        "    # We first convolve the image with 7x7 filters, to be able to better extract\n",
        "    # low-level features such as contours. Using conv with stride=2 halves the\n",
        "    # resolution of the input, reducing considerably the computation cost, and\n",
        "    # increasing the receptive field.  \n",
        "    self.initial_conv = hk.Conv2D(\n",
        "        output_channels=64,\n",
        "        kernel_shape=7,\n",
        "        stride=2,\n",
        "        with_bias=False,\n",
        "        padding=\"SAME\",\n",
        "        name=\"initial_conv\")\n",
        "\n",
        "    self.block_groups = []\n",
        "    strides = (1, 2, 2, 2)\n",
        "    for i in range(4):\n",
        "      self.block_groups.append(\n",
        "          BlockGroup(channels=channels_per_group[i],\n",
        "                     num_blocks=blocks_per_group[i],\n",
        "                     stride=strides[i],\n",
        "                     bn_config=bn_config,\n",
        "                     bottleneck=bottleneck,\n",
        "                     use_projection=use_projection[i],\n",
        "                     name=\"block_group_%d\" % (i)))\n",
        "\n",
        "    self.final_batchnorm = hk.BatchNorm(name=\"final_batchnorm\", **bn_config)\n",
        "    self.logits = hk.Linear(num_classes, w_init=jnp.zeros, name=\"logits\")\n",
        "\n",
        "  def __call__(self, inputs, is_training, test_local_stats=False):\n",
        "    out = inputs\n",
        "    out = self.initial_conv(out)\n",
        "    # Reduce the spatial resolution of the activations by a factor of 2. This\n",
        "    # increases the receptive field and reduces the computation cost. Note that\n",
        "    # compared to a strided conv which has the same effects, the pooling layers \n",
        "    # does not have trainable parameters.\n",
        "    out = hk.max_pool(out,\n",
        "                      window_shape=(1, 3, 3, 1),\n",
        "                      strides=(1, 2, 2, 1),\n",
        "                      padding=\"SAME\")\n",
        "\n",
        "    for block_group in self.block_groups:\n",
        "      out = block_group(out, is_training, test_local_stats)\n",
        "\n",
        "    out = self.final_batchnorm(out, is_training, test_local_stats)\n",
        "    out = jax.nn.relu(out)\n",
        "\n",
        "    # Pool over spatial dimensions to obtain the final vector embedding\n",
        "    # of the image. Use jnp.mean and not hk.avg_pool, to make sure that the\n",
        "    # network can be applied to inputs with any resolution without modification\n",
        "    # of the model.\n",
        "    out = jnp.mean(out, axis=[1, 2])\n",
        "    return self.logits(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOmfZ5MND7sL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Instantiate Resnet18\n",
        "class ResNet18(ResNet):\n",
        "  \"\"\"ResNet18.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               num_classes: int,\n",
        "               bn_config: Optional[Mapping[str, float]] = None,\n",
        "               name: Optional[str] = None):\n",
        "    \"\"\"Constructs a ResNet model.\n",
        "    Args:\n",
        "      num_classes: The number of classes to classify the inputs into.\n",
        "      bn_config: A dictionary of two elements, `decay_rate` and `eps` to be\n",
        "        passed on to the `BatchNorm` layers.\n",
        "      name: Name of the module.\n",
        "    \"\"\"\n",
        "    super().__init__(blocks_per_group=(2, 2, 2, 2),\n",
        "                     num_classes=num_classes,\n",
        "                     bn_config=bn_config,\n",
        "                     bottleneck=False,\n",
        "                     channels_per_group=(64, 128, 256, 512),\n",
        "                     use_projection=(False, True, True, True),\n",
        "                     name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJzMPIQpKwBQ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create the forward pass of the model\n",
        "def net_fn(\n",
        "    batch: Batch,\n",
        "    is_training: bool,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Forward pass of the resnet.\"\"\"\n",
        "  model = ResNet18(num_classes, bn_config={'decay_rate': model_bn_decay})\n",
        "  return model(batch['image'], is_training=is_training)\n",
        "\n",
        "# Transform the forward function into a pair of pure functions.\n",
        "# We use transform with state because we need to keep the state of the network,\n",
        "# e.g. for batch norm statistics.\n",
        "net = hk.transform_with_state(net_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe_QUoklDdLC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define learning rate schedule and optimizer\n",
        "# We use learning rate annealing during training. We start with a larger\n",
        "# learning rate `lr_init` which allows exploring faster the space of solutions\n",
        "# and we reduce it by a factor of 10 `lr_factor` after a predefined number of\n",
        "# steps. Smaller learning rate at the end of the training allows the model to\n",
        "# explore a local neighbourhood and settle on a good local minimum. \n",
        "def lr_schedule(step: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Define learning rate annealing schedule.\"\"\"\n",
        "  # After how many steps to apply the learning rate reduction\n",
        "  boundaries = jnp.array((200e3, 300e3, 350e3))\n",
        "  # Every time we hit a predefined number of steps, we apply the reduction\n",
        "  # of the learning rate by `lr_factor`\n",
        "  lr_decay_exponent = jnp.sum(step >= boundaries)\n",
        "  lr_init = 0.1\n",
        "  lr_factor = 0.1\n",
        "  return lr_init * lr_factor**lr_decay_exponent\n",
        "\n",
        "# Define the optimiser, we use SGD with momentum\n",
        "def make_optimizer():\n",
        "  \"\"\"SGD with nesterov momentum and a custom lr schedule.\"\"\"\n",
        "  return optix.chain(optix.trace(decay=optimizer_momentum,\n",
        "                                 nesterov=optimizer_use_nesterov),\n",
        "                     optix.scale_by_schedule(lr_schedule),\n",
        "                     optix.scale(-1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKURlFI8Lkgb",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Define the loss function: cross-entropy for classification and weight decay for regularization\n",
        "# Function to compute l2 loss - useful for regularisation\n",
        "def l2_loss(params: Iterable[jnp.ndarray]) -> jnp.ndarray:\n",
        "  return 0.5 * sum(jnp.sum(jnp.square(p)) for p in params)\n",
        "\n",
        "# Function to compute softmax cross entropy for classification\n",
        "def softmax_cross_entropy(\n",
        "    *,\n",
        "    logits: jnp.ndarray,\n",
        "    labels: jnp.ndarray,\n",
        ") -> jnp.ndarray:\n",
        "  return -jnp.sum(labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "\n",
        "def loss_fn(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    batch: Batch,\n",
        ") -> Tuple[jnp.ndarray, hk.State]:\n",
        "  \"\"\"Computes a regularized loss for the given batch.\"\"\"\n",
        "  # The third parameter would be an rng key if one is needed in running\n",
        "  # the model, e.g. for dropout. If not needed, pass `None`.\n",
        "  logits, state = net.apply(params, state, None, batch, is_training=True)\n",
        "  # The labels are given as class indices; convert to one_hot representation\n",
        "  labels = jax.nn.one_hot(batch['label'], num_classes)\n",
        "  # Compute classification loss\n",
        "  cat_loss = jnp.mean(softmax_cross_entropy(logits=logits, labels=labels))\n",
        "  # Get all the trainable parameters of the model, except batch_norm parameters\n",
        "  # to apply weight decay regularisation , i.e. we penalise weights with\n",
        "  # large magnitude\n",
        "  l2_params = [p for ((mod_name, _), p) in tree.flatten_with_path(params)\n",
        "               if 'batchnorm' not in mod_name]\n",
        "  # We apply a weighting factor to the regularisation loss, so that it does\n",
        "  # not dominate the total loss\n",
        "  reg_loss = train_weight_decay * l2_loss(l2_params)\n",
        "  # Compute the final loss\n",
        "  loss = cat_loss + reg_loss\n",
        "  return loss, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esBIlJ5PMAb1",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Define the training step and training dataset\n",
        "@jax.jit\n",
        "def train_step(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    opt_state: OptState,\n",
        "    batch: Batch, \n",
        ") -> Tuple[hk.Params, hk.State, OptState, Scalars]:\n",
        "  \"\"\"Applies an update to parameters and returns new state.\"\"\"\n",
        "  (loss, state), grads = (\n",
        "      jax.value_and_grad(loss_fn, has_aux=True)(params, state, batch))\n",
        "\n",
        "  # Compute and apply updates via our optimizer.\n",
        "  updates, opt_state = make_optimizer().update(grads, opt_state)\n",
        "  params = optix.apply_updates(params, updates)\n",
        "\n",
        "  return params, state, opt_state, loss\n",
        "\n",
        "# Get training dataset\n",
        "train_dataset = load(train_split, is_training=True, batch_size=train_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOhYokJEMQOS",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Define the evaluation\n",
        "@jax.jit\n",
        "def eval_batch(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    batch: Batch,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Evaluates a batch.\"\"\"\n",
        "  # The third parameter would be an rng key if one is needed in running the model,\n",
        "  # e.g. for dropout. If not needed, pass `None`.\n",
        "  logits, _ = net.apply(params, state, None, batch, is_training=False)\n",
        "  predicted_label = jnp.argmax(logits, axis=-1)\n",
        "  correct = jnp.sum(jnp.equal(predicted_label, batch['label']))\n",
        "  return correct.astype(jnp.float32)\n",
        "\n",
        "def evaluate(\n",
        "    split: str,\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        ") -> Scalars:\n",
        "  \"\"\"Evaluates the model at the given params/state.\"\"\"\n",
        "  test_dataset = load(split, is_training=False, batch_size=eval_batch_size)\n",
        "  correct = jnp.array(0)\n",
        "  total = 0\n",
        "  for eval_iter in range(num_eval_steps):\n",
        "    correct += eval_batch(params, state, next(test_dataset))\n",
        "    total += eval_batch_size\n",
        "  return correct.item() / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lspc5ixwPk3Q",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Initialise the model and the optimiser\n",
        "def make_initial_state(\n",
        "    rng: jnp.ndarray,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, hk.State, OptState]:\n",
        "  \"\"\"Computes the initial network state.\"\"\"\n",
        "  params, state = net.init(rng, batch, is_training=True)\n",
        "  opt_state = make_optimizer().init(params)\n",
        "  return params, state, opt_state\n",
        "\n",
        "# We need a random key for initialization\n",
        "rng = jax.random.PRNGKey(train_init_random_seed)\n",
        "\n",
        "# Initialization requires an example input to calculate shapes of parameters.\n",
        "batch = next(train_dataset)\n",
        "params, state, opt_state = make_initial_state(rng, batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izeQeUlfC5Xh",
        "colab_type": "text"
      },
      "source": [
        "### Increase in the number of parameters compared to baseline (less than 1%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ADgy1kdcO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_params(params: hk.Params):\n",
        "  num_params = 0\n",
        "  for p in jax.tree_leaves(params): \n",
        "    # print(p.shape)\n",
        "    num_params = num_params + jnp.prod(p.shape)\n",
        "  return num_params\n",
        "print('Total number of parameters %d' % get_num_params(params))\n",
        "\n",
        "num_params_baseline = 11179850\n",
        "increase = (get_num_params(params) - num_params_baseline) / num_params_baseline\n",
        "\n",
        "print('Increase in number of parameters %.2f%%' %(increase*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZkweB3XMpN-",
        "colab_type": "text"
      },
      "source": [
        "### Run training loop and evaluation; full training gives accuracy ~90.3%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH3mhqv_VUGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_every = train_eval_every\n",
        "log_every = train_log_every\n",
        "\n",
        "for step_num in range(int(num_train_steps)):\n",
        "  # Take a training step.\n",
        "  params, state, opt_state, train_loss = (\n",
        "      train_step(params, state, opt_state, next(train_dataset)))\n",
        "\n",
        "  # We run evaluation during training to see the progress.\n",
        "  if eval_every > 0 and step_num % eval_every == 0:\n",
        "    eval_acc = evaluate(eval_split, params, state)\n",
        "    print('[Eval acc %s/%s] %s'%(step_num, int(num_train_steps), eval_acc))\n",
        "\n",
        "  # Log progress at fixed intervals.\n",
        "  if step_num % log_every == 0:\n",
        "    print('[Train loss %s/%s] %s'%(step_num, int(num_train_steps), train_loss))\n",
        "\n",
        "# Once training has finished we run eval one more time to get final results.\n",
        "eval_acc = evaluate(eval_split, params, state)\n",
        "print('[Eval acc FINAL]: %s'%(eval_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}