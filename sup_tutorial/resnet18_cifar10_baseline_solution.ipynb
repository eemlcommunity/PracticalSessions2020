{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet18_cifar10_baseline_solution.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTxKxH81PcNz",
        "colab_type": "text"
      },
      "source": [
        "# Supervised learning tutorial\n",
        "\n",
        "In supervised learning, we are given a set of (input, label) pairs `(x_i, y_i)`, and the goal is to learn a function that maps the inputs to the correct labels. The hope is that, once learnt, this function would generalise to samples that were not seen during training.\n",
        "\n",
        "In this tutorial, we will focus on the task of image classification and we use Cifar10 dataset and Resnet18 neural architecture to learn the desired mapping (Part 1). \n",
        "\n",
        "Once the model is trained, we will perform an [adversarial attack in pixel space](https://arxiv.org/pdf/1312.6199.pdf) on this classifier to highlight some of the weaknesses of these models (Part 2).\n",
        "\n",
        "Finally, we will use a self-attention mechanism ([Squeeze-and-Excitation](https://arxiv.org/pdf/1709.01507.pdf)) to improve the generalisation power of the model (Part 3). *Note*: Although the test accuracy is improved, the robustness to adversarial attacks is not necessarily improved.\n",
        "\n",
        "**Key takeaways**\n",
        "\n",
        "By the end of this tutorial, you will know:\n",
        "* how to implement (using Jax and haiku libraries) a residual convolutional neural network for image classification and how to train it on an image dataset, using standard data augmentation, with weight decay regularisation and batch normalisation\n",
        "* how to write new network modules\n",
        "* how to use the same backpropagation algorithm that was initially used for training the network, to build adversarial examples that fool the network\n",
        "* how to add new loss terms (to perform the adversarial attack)\n",
        "* how to implement a simple, yet effective self-attention mechanism to improve classification accuracy.\n",
        "\n",
        "**Homework**\n",
        "\n",
        "Test the classifier's robustness to changes in the input distribution in the geometric space (e.g. by applying rotations to the inputs). What do you observe? How can the observed behaviour be prevented (at least partially)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGuivbg9OJ-w",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Resnet18 classifier on Cifar10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUhxehxYHuuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Iterable, Mapping, Tuple, Generator, Optional, Sequence, Text, Union, List\n",
        "\n",
        "# We will use haiku on top of jax; it is not included by default, so let's install it  \n",
        "!pip install -q dm-haiku\n",
        "import haiku as hk\n",
        "\n",
        "import jax\n",
        "from jax.experimental import optix  # package for optimizer\n",
        "import jax.numpy as jnp  # equivalent of numpy on GPU and TPU\n",
        "import numpy as np  # original numpy\n",
        "\n",
        "!pip install -q dm-tree\n",
        "import tree\n",
        "import enum\n",
        "import time\n",
        "\n",
        "# Dataset libraries\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Plotting library.\n",
        "from matplotlib import pyplot as plt\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYTu486HJS6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define useful types.\n",
        "OptState = Tuple[optix.TraceState, optix.ScaleByScheduleState, optix.ScaleState]\n",
        "Scalars = Mapping[str, jnp.ndarray]\n",
        "Batch = Mapping[Text, np.ndarray]\n",
        "ClassNames = Mapping[List, str]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah2fsQ2DI1IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset constants for cifar10 dataset, the \"MNIST of real images\":\n",
        "# it contains low-res natural images (32x32x3) belonging to 10 classes.\n",
        "dataset_name = 'cifar10'\n",
        "class_cifar10 = [u'airplane', u'automobile', u'bird', u'cat', u'deer', u'dog', u'frog', u'horse', u'ship', u'truck'] \n",
        "train_split = 'train'\n",
        "eval_split = 'test'\n",
        "num_examples = {train_split: 50000,\n",
        "                eval_split: 10000}\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maQt-JG5HChv",
        "colab_type": "text"
      },
      "source": [
        "### Hyper-parameters for training and optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD0PWPguI_Xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size = 128 #@param\n",
        "eval_batch_size = 100  #@param\n",
        "model_bn_decay = 0.9  #@param\n",
        "train_weight_decay = 1e-4  #@param\n",
        "optimizer_momentum = 0.9  #@param\n",
        "optimizer_use_nesterov = True  #@param\n",
        "train_eval_every = 1000  #@param\n",
        "train_init_random_seed = 42  #@param\n",
        "train_log_every = 100  #@param\n",
        "num_train_steps = 400e3  #@param\n",
        "num_eval_steps = (num_examples[eval_split]) // eval_batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI1xFR8eGr5p",
        "colab_type": "text"
      },
      "source": [
        "### Dataset loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPD4on8RD2Z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use tensorflow readers; JAX does not have support for input data reading\n",
        "# and pre-processing.\n",
        "def load(split: str,\n",
        "         *,\n",
        "         is_training: bool,\n",
        "         batch_size: int) -> Generator[Batch, None, None]:\n",
        "  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
        "  ds = tfds.load('cifar10', split=split).cache().repeat()\n",
        "  \n",
        "  if is_training:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "\n",
        "  # Define the preprocessing for each train and test image\n",
        "  def preprocess(example):\n",
        "    image = _preprocess_image(example['image'], is_training)\n",
        "    return {'image': image, 'label': example['label']}\n",
        "\n",
        "  # Apply the preprocessing function to all samples in a batch using `map`\n",
        "  ds = ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Get samples grouped in mini-batches to train using SGD\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return tfds.as_numpy(ds)  # return numpy array\n",
        "\n",
        "def _preprocess_image(\n",
        "    image: tf.Tensor,\n",
        "    is_training: bool,\n",
        ") -> tf.Tensor:\n",
        "  \"\"\"Returns processed and resized image.\"\"\"\n",
        "  # Images are stored as uint8; we convert to float for further processing.\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  # Normalise pixel values between -1 and 1: original images are in [0, 255].\n",
        "  # We normalise to [-1, 1] to have 0 mean and unit variance in the inputs,\n",
        "  # as it makes the training more stable. Note that we do this normalisation \n",
        "  # over the activations of all the layers in the network by using batch \n",
        "  # normalisation layers.\n",
        "  image = 2 * (image / 255.0) - 1.0\n",
        "\n",
        "  # During training, we use data augmentation (left-right flips, random crops).\n",
        "  # In this way, we are effectively increasing the size of the training dataset,\n",
        "  # leading to improved generalisation.\n",
        "  if is_training:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    # Pad images by reflecting the boundaries and randomly sample a 32x32 patch.\n",
        "    image = tf.pad(image, [[4, 4], [4, 4], [0, 0]], mode='REFLECT')\n",
        "    image = tf.image.random_crop(image, size=(32, 32, 3))\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1mUrFWUMZ7p",
        "colab_type": "text"
      },
      "source": [
        "### Function to display images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HtjHZ3FMeoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_IMAGES = 8\n",
        "def gallery(images: np.ndarray,\n",
        "            label: np.ndarray,\n",
        "            class_names: ClassNames=class_cifar10,\n",
        "            title: str='Input images'):  \n",
        "  \"\"\"Display a batch of images.\"\"\"\n",
        "  num_frames, h, w, num_channels = images.shape\n",
        "  num_frames = min(num_frames, MAX_IMAGES)\n",
        "  ff, axes = plt.subplots(1, num_frames,\n",
        "                          figsize=(32, 32),\n",
        "                          subplot_kw={'xticks': [], 'yticks': []})\n",
        "  if images.min() < 0:\n",
        "    images = (images + 1.) / 2.\n",
        "  for i in range(0, num_frames):\n",
        "    if num_channels == 3:\n",
        "      axes[i].imshow(np.squeeze(images[i]))\n",
        "    else:\n",
        "      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "    axes[i].set_title(class_names[label[i]], fontsize=28)\n",
        "    plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "    plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "  ff.subplots_adjust(wspace=0.1)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j5qHouauJeS",
        "colab_type": "text"
      },
      "source": [
        "### Create a resnet block (coding exercise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbSRNkW-uFmL",
        "colab_type": "text"
      },
      "source": [
        "In a typical sequential model (no branching), the network as a whole is optimised to find the mapping between inputs and correct labels. In residual networks, each layer can learn an additive residual representation wrt to the representation already computed up to the previous layer, making the optimisation easier.\n",
        "\n",
        "As opposed to [resnet-v1](https://arxiv.org/pdf/1512.03385.pdf) blocks (left), [resnet-v2](https://arxiv.org/pdf/1603.05027.pdf) blocks (right) use pre-activation modules, i.e. the batch normalisation (`BN`) and relu (`ReLU`) nonlinearity are applied within the resnet block, before the convolutional layer (`weight`). This allows the model to learn identity mappings over the shortcuts throughout the network, improving further the backpropagation of gradients.   \n",
        "\n",
        "<img src=\"https://github.com/eemlcommunity/PracticalSessions2020/blob/master/assets/v1v2.png?raw=true\" alt=\"resnet blocks\" style=\"width: 80px;\"/>\n",
        "\n",
        "Figure from original [resnet-v2 paper](https://arxiv.org/pdf/1603.05027.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZa53uq8CEx",
        "colab_type": "text"
      },
      "source": [
        "*Bottleneck blocks*: To reduce the number of parameters and memory footprint without sacrificing expressivity, bottleneck blocks can be applied. Instead of using 2 conv layers (`weight` in the figure above) with 3x3 filters, empirically it is shown that projecting in a lower dimensional space (using 1x1 conv layers), applying 3x3 convolutions, and then reprojecting back into the original dimension space, does not affect accuracy.    \n",
        "\n",
        "*1x1 conv shortcuts*: when the input and the output of a resnet block have different numbers of channels, 1x1 convolutional layers are used on the shortcut to project the representation to the desired output feature dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enwxoMEatMQM",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def check_length(length, value, name):\n",
        "  if len(value) != length:\n",
        "    raise ValueError(f\"`{name}` must be of length {length} not {len(value)}\")\n",
        "\n",
        "class BlockV2(hk.Module):\n",
        "  \"\"\"ResNet V2 block with optional bottleneck.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      channels: int,\n",
        "      stride: Union[int, Sequence[int]],\n",
        "      use_projection: bool,\n",
        "      bn_config: Mapping[str, float],\n",
        "      bottleneck: bool,\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self.use_projection = use_projection\n",
        "\n",
        "    # Define batch norm parameters: the batch_norm layer normalises the inputs \n",
        "    # to have zero mean and unit variance. To not affect the expressivity\n",
        "    # of the network, e.g. in cases where it would be better for the activations\n",
        "    # to not be 0-centred or to have larger variance, batch_norm can optionally\n",
        "    # learn a scale and an offset parameters. \n",
        "    bn_config = dict(bn_config)\n",
        "    bn_config.setdefault(\"create_scale\", True)\n",
        "    bn_config.setdefault(\"create_offset\", True)\n",
        "\n",
        "    # See comment above about 1x1 conv shortcut \n",
        "    if self.use_projection:\n",
        "      self.proj_conv = hk.Conv2D(\n",
        "          output_channels=channels,\n",
        "          kernel_shape=1,\n",
        "          stride=stride,\n",
        "          with_bias=False,\n",
        "          padding=\"SAME\",\n",
        "          name=\"shortcut_conv\")\n",
        "\n",
        "    # If we use bottleneck blocks (see comment above), inside the resnet block \n",
        "    # we first project the activations into a lower dimensional space, \n",
        "    # which has number of channels divided by `channel_div` compared to the \n",
        "    # desired number of channels in the output.\n",
        "    channel_div = 4 if bottleneck else 1\n",
        "    conv_0 = hk.Conv2D(\n",
        "        output_channels=channels // channel_div,\n",
        "        kernel_shape=1 if bottleneck else 3,\n",
        "        stride=1,\n",
        "        with_bias=False,\n",
        "        padding=\"SAME\",\n",
        "        name=\"conv_0\")\n",
        "\n",
        "    bn_0 = hk.BatchNorm(name=\"batchnorm_0\", **bn_config)\n",
        "    # Then we apply the 3x3 conv layer\n",
        "    conv_1 = hk.Conv2D(\n",
        "        output_channels=channels // channel_div,\n",
        "        kernel_shape=3,\n",
        "        stride=stride,\n",
        "        with_bias=False,\n",
        "        padding=\"SAME\",\n",
        "        name=\"conv_1\")\n",
        "\n",
        "    bn_1 = hk.BatchNorm(name=\"batchnorm_1\", **bn_config)\n",
        "    layers = ((conv_0, bn_0), (conv_1, bn_1))\n",
        "\n",
        "    # When using bottleneck, we have also a 3rd 1x1 convolutional layer\n",
        "    # within the resnet block (see comment above about bottleneck blocks)\n",
        "    if bottleneck:\n",
        "      conv_2 = hk.Conv2D(\n",
        "          output_channels=channels,\n",
        "          kernel_shape=1,\n",
        "          stride=1,\n",
        "          with_bias=False,\n",
        "          padding=\"SAME\",\n",
        "          name=\"conv_2\")\n",
        "\n",
        "      bn_2 = hk.BatchNorm(name=\"batchnorm_2\", **bn_config)\n",
        "      layers = layers + ((conv_2, bn_2),)\n",
        "\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, inputs, is_training, test_local_stats):\n",
        "    x = shortcut = inputs\n",
        "    ######################\n",
        "    ### YOUR CODE HERE ###\n",
        "    ######################\n",
        "    for i, (conv_i, bn_i) in enumerate(self.layers):\n",
        "      # Apply pre-activation: batch_norm + relu\n",
        "      x = bn_i(x, is_training, test_local_stats)\n",
        "      x = jax.nn.relu(x)\n",
        "      # If using 1x1 conv projection on the shortcut, apply proj_conv once \n",
        "      if i == 0 and self.use_projection:\n",
        "        shortcut = self.proj_conv(x)\n",
        "      # Apply convolution\n",
        "      x = conv_i(x)\n",
        "\n",
        "    return x + shortcut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu3_qgA-7N0G",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Stack resnet blocks\n",
        "class BlockGroup(hk.Module):\n",
        "  \"\"\"Group of blocks for ResNet implementation.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      channels: int,\n",
        "      num_blocks: int,\n",
        "      stride: Union[int, Sequence[int]],\n",
        "      bn_config: Mapping[str, float],\n",
        "      bottleneck: bool,\n",
        "      use_projection: bool,\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self.blocks = []\n",
        "    for i in range(num_blocks):\n",
        "      self.blocks.append(\n",
        "          BlockV2(channels=channels,\n",
        "                  stride=(1 if i else stride),\n",
        "                  use_projection=(i == 0 and use_projection),\n",
        "                  bottleneck=bottleneck,\n",
        "                  bn_config=bn_config,\n",
        "                  name=\"block_%d\" % (i)))\n",
        "\n",
        "  def __call__(self, inputs, is_training, test_local_stats):\n",
        "    out = inputs\n",
        "    for block in self.blocks:\n",
        "      out = block(out, is_training, test_local_stats)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef_YOIYiDy7H",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define a generic resnet architecture\n",
        "# Note: This class is generic, it can be used to instantiate any Resnet \n",
        "# model, e.g. Resnet-50, Resnet-101, etc. by substituting the correct block\n",
        "# parameters \n",
        "class ResNet(hk.Module):\n",
        "  \"\"\"ResNet model.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      blocks_per_group: Sequence[int],\n",
        "      num_classes: int,\n",
        "      bn_config: Optional[Mapping[str, float]] = None,\n",
        "      bottleneck: bool = True,\n",
        "      channels_per_group: Sequence[int] = (256, 512, 1024, 2048),\n",
        "      use_projection: Sequence[bool] = (True, True, True, True),\n",
        "      name: Optional[str] = None,\n",
        "  ):\n",
        "    \"\"\"Constructs a ResNet model.\n",
        "    Args:\n",
        "      blocks_per_group: A sequence of length 4 that indicates the number of\n",
        "        blocks created in each group.\n",
        "      num_classes: The number of classes to classify the inputs into.\n",
        "      bn_config: A dictionary of two elements, `decay_rate` and `eps` to be\n",
        "        passed on to the `BatchNorm` layers. By default the `decay_rate` is\n",
        "        `0.9` and `eps` is `1e-5`.\n",
        "      bottleneck: Whether the block should bottleneck or not. Defaults to True.\n",
        "      channels_per_group: A sequence of length 4 that indicates the number\n",
        "        of channels used for each block in each group.\n",
        "      use_projection: A sequence of length 4 that indicates whether each\n",
        "        residual block should use projection.\n",
        "      name: Name of the module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    bn_config = dict(bn_config or {})\n",
        "    bn_config.setdefault(\"decay_rate\", 0.9)\n",
        "    bn_config.setdefault(\"eps\", 1e-5)\n",
        "    bn_config.setdefault(\"create_scale\", True)\n",
        "    bn_config.setdefault(\"create_offset\", True)\n",
        "\n",
        "    # Number of blocks in each group for ResNet.\n",
        "    check_length(4, blocks_per_group, \"blocks_per_group\")\n",
        "    check_length(4, channels_per_group, \"channels_per_group\")\n",
        "\n",
        "    # We first convolve the image with 7x7 filters, to be able to better extract\n",
        "    # low-level features such as contours. Using conv with stride=2 halves the\n",
        "    # resolution of the input, reducing considerably the computation cost, and\n",
        "    # increasing the receptive field.  \n",
        "    self.initial_conv = hk.Conv2D(\n",
        "        output_channels=64,\n",
        "        kernel_shape=7,\n",
        "        stride=2,\n",
        "        with_bias=False,\n",
        "        padding=\"SAME\",\n",
        "        name=\"initial_conv\")\n",
        "\n",
        "    self.block_groups = []\n",
        "    strides = (1, 2, 2, 2)\n",
        "    for i in range(4):\n",
        "      self.block_groups.append(\n",
        "          BlockGroup(channels=channels_per_group[i],\n",
        "                     num_blocks=blocks_per_group[i],\n",
        "                     stride=strides[i],\n",
        "                     bn_config=bn_config,\n",
        "                     bottleneck=bottleneck,\n",
        "                     use_projection=use_projection[i],\n",
        "                     name=\"block_group_%d\" % (i)))\n",
        "\n",
        "    self.final_batchnorm = hk.BatchNorm(name=\"final_batchnorm\", **bn_config)\n",
        "    self.logits = hk.Linear(num_classes, w_init=jnp.zeros, name=\"logits\")\n",
        "\n",
        "  def __call__(self, inputs, is_training, test_local_stats=False):\n",
        "    out = inputs\n",
        "    out = self.initial_conv(out)\n",
        "    # Reduce the spatial resolution of the activations by a factor of 2. This\n",
        "    # increases the receptive field and reduces the computation cost. Note that\n",
        "    # compared to a strided conv which has the same effects, the pooling layers \n",
        "    # does not have trainable parameters.\n",
        "    out = hk.max_pool(out,\n",
        "                      window_shape=(1, 3, 3, 1),\n",
        "                      strides=(1, 2, 2, 1),\n",
        "                      padding=\"SAME\")\n",
        "\n",
        "    for block_group in self.block_groups:\n",
        "      out = block_group(out, is_training, test_local_stats)\n",
        "\n",
        "    out = self.final_batchnorm(out, is_training, test_local_stats)\n",
        "    out = jax.nn.relu(out)\n",
        "\n",
        "    # Pool over spatial dimensions to obtain the final vector embedding\n",
        "    # of the image. Use jnp.mean and not hk.avg_pool, to make sure that the\n",
        "    # network can be applied to inputs with any resolution without modification\n",
        "    # of the model.\n",
        "    out = jnp.mean(out, axis=[1, 2])\n",
        "    return self.logits(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOmfZ5MND7sL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Instantiate Resnet18\n",
        "class ResNet18(ResNet):\n",
        "  \"\"\"ResNet18.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               num_classes: int,\n",
        "               bn_config: Optional[Mapping[str, float]] = None,\n",
        "               name: Optional[str] = None):\n",
        "    \"\"\"Constructs a ResNet model.\n",
        "    Args:\n",
        "      num_classes: The number of classes to classify the inputs into.\n",
        "      bn_config: A dictionary of two elements, `decay_rate` and `eps` to be\n",
        "        passed on to the `BatchNorm` layers.\n",
        "      name: Name of the module.\n",
        "    \"\"\"\n",
        "    super().__init__(blocks_per_group=(2, 2, 2, 2),\n",
        "                     num_classes=num_classes,\n",
        "                     bn_config=bn_config,\n",
        "                     bottleneck=False,\n",
        "                     channels_per_group=(64, 128, 256, 512),\n",
        "                     use_projection=(False, True, True, True),\n",
        "                     name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH1DDv9fLq2G",
        "colab_type": "text"
      },
      "source": [
        "### Create the forward pass of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJzMPIQpKwBQ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def net_fn(\n",
        "    batch: Batch,\n",
        "    is_training: bool,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Forward pass of the resnet.\"\"\"\n",
        "  model = ResNet18(num_classes, bn_config={'decay_rate': model_bn_decay})\n",
        "  return model(batch['image'], is_training=is_training)\n",
        "\n",
        "# Transform the forward function into a pair of pure functions.\n",
        "# We use transform with state because we need to keep the state of the network,\n",
        "# e.g. for batch norm statistics.\n",
        "net = hk.transform_with_state(net_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_6OQHRaUa2z",
        "colab_type": "text"
      },
      "source": [
        "### Define learning rate schedule and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe_QUoklDdLC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# We use learning rate annealing during training. We start with a larger\n",
        "# learning rate `lr_init` which allows exploring faster the space of solutions\n",
        "# and we reduce it by a factor of 10 `lr_factor` after a predefined number of\n",
        "# steps. Smaller learning rate at the end of the training allows the model to\n",
        "# explore a local neighbourhood and settle on a good local minimum. \n",
        "def lr_schedule(step: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Define learning rate annealing schedule.\"\"\"\n",
        "  # After how many steps to apply the learning rate reduction\n",
        "  boundaries = jnp.array((200e3, 300e3, 350e3))\n",
        "  # Every time we hit a predefined number of steps, we apply the reduction\n",
        "  # of the learning rate by `lr_factor`\n",
        "  lr_decay_exponent = jnp.sum(step >= boundaries)\n",
        "  lr_init = 0.1\n",
        "  lr_factor = 0.1\n",
        "  return lr_init * lr_factor**lr_decay_exponent\n",
        "\n",
        "# Define the optimiser, we use SGD with momentum\n",
        "def make_optimizer():\n",
        "  \"\"\"SGD with nesterov momentum and a custom lr schedule.\"\"\"\n",
        "  return optix.chain(optix.trace(decay=optimizer_momentum,\n",
        "                                 nesterov=optimizer_use_nesterov),\n",
        "                     optix.scale_by_schedule(lr_schedule),\n",
        "                     optix.scale(-1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WoUR6bGLt11",
        "colab_type": "text"
      },
      "source": [
        "### Define the loss function: cross-entropy for classification and weight decay for regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKURlFI8Lkgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to compute l2 loss - useful for regularisation\n",
        "def l2_loss(params: Iterable[jnp.ndarray]) -> jnp.ndarray:\n",
        "  return 0.5 * sum(jnp.sum(jnp.square(p)) for p in params)\n",
        "\n",
        "# Function to compute softmax cross entropy for classification\n",
        "def softmax_cross_entropy(\n",
        "    *,\n",
        "    logits: jnp.ndarray,\n",
        "    labels: jnp.ndarray,\n",
        ") -> jnp.ndarray:\n",
        "  return -jnp.sum(labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "\n",
        "def loss_fn(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    batch: Batch,\n",
        ") -> Tuple[jnp.ndarray, hk.State]:\n",
        "  \"\"\"Computes a regularized loss for the given batch.\"\"\"\n",
        "  # The third parameter would be an rng key if one is needed in running\n",
        "  # the model, e.g. for dropout. If not needed, pass `None`.\n",
        "  logits, state = net.apply(params, state, None, batch, is_training=True)\n",
        "  # The labels are given as class indices; convert to one_hot representation\n",
        "  labels = jax.nn.one_hot(batch['label'], num_classes)\n",
        "  # Compute classification loss\n",
        "  cat_loss = jnp.mean(softmax_cross_entropy(logits=logits, labels=labels))\n",
        "  # Get all the trainable parameters of the model, except batch_norm parameters\n",
        "  # to apply weight decay regularisation , i.e. we penalise weights with\n",
        "  # large magnitude\n",
        "  l2_params = [p for ((mod_name, _), p) in tree.flatten_with_path(params)\n",
        "               if 'batchnorm' not in mod_name]\n",
        "  # We apply a weighting factor to the regularisation loss, so that it does\n",
        "  # not dominate the total loss\n",
        "  reg_loss = train_weight_decay * l2_loss(l2_params)\n",
        "  # Compute the final loss\n",
        "  loss = cat_loss + reg_loss\n",
        "  return loss, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H141qefwL5sg",
        "colab_type": "text"
      },
      "source": [
        "### Define the training step and training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esBIlJ5PMAb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def train_step(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    opt_state: OptState,\n",
        "    batch: Batch, \n",
        ") -> Tuple[hk.Params, hk.State, OptState, Scalars]:\n",
        "  \"\"\"Applies an update to parameters and returns new state.\"\"\"\n",
        "  (loss, state), grads = (\n",
        "      jax.value_and_grad(loss_fn, has_aux=True)(params, state, batch))\n",
        "\n",
        "  # Compute and apply updates via our optimizer.\n",
        "  updates, opt_state = make_optimizer().update(grads, opt_state)\n",
        "  params = optix.apply_updates(params, updates)\n",
        "\n",
        "  return params, state, opt_state, loss\n",
        "\n",
        "# Get training dataset\n",
        "train_dataset = load(train_split, is_training=True, batch_size=train_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE8NfsJaMMNK",
        "colab_type": "text"
      },
      "source": [
        "### Define the evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOhYokJEMQOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def eval_batch(\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        "    batch: Batch,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Evaluates a batch.\"\"\"\n",
        "  # The third parameter would be an rng key if one is needed in running the model,\n",
        "  # e.g. for dropout. If not needed, pass `None`.\n",
        "  logits, _ = net.apply(params, state, None, batch, is_training=False)\n",
        "  predicted_label = jnp.argmax(logits, axis=-1)\n",
        "  correct = jnp.sum(jnp.equal(predicted_label, batch['label']))\n",
        "  return correct.astype(jnp.float32)\n",
        "\n",
        "def evaluate(\n",
        "    split: str,\n",
        "    params: hk.Params,\n",
        "    state: hk.State,\n",
        ") -> Scalars:\n",
        "  \"\"\"Evaluates the model at the given params/state.\"\"\"\n",
        "  test_dataset = load(split, is_training=False, batch_size=eval_batch_size)\n",
        "  correct = jnp.array(0)\n",
        "  total = 0\n",
        "  for eval_iter in range(num_eval_steps):\n",
        "    correct += eval_batch(params, state, next(test_dataset))\n",
        "    total += eval_batch_size\n",
        "  return correct.item() / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1psavTyYNZ8p",
        "colab_type": "text"
      },
      "source": [
        "### Initialise the model and the optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lspc5ixwPk3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_initial_state(\n",
        "    rng: jnp.ndarray,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, hk.State, OptState]:\n",
        "  \"\"\"Computes the initial network state.\"\"\"\n",
        "  params, state = net.init(rng, batch, is_training=True)\n",
        "  opt_state = make_optimizer().init(params)\n",
        "  return params, state, opt_state\n",
        "\n",
        "# We need a random key for initialization\n",
        "rng = jax.random.PRNGKey(train_init_random_seed)\n",
        "\n",
        "# Initialization requires an example input to calculate shapes of parameters.\n",
        "batch = next(train_dataset)\n",
        "params, state, opt_state = make_initial_state(rng, batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izeQeUlfC5Xh",
        "colab_type": "text"
      },
      "source": [
        "### How many parameters in your model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ADgy1kdcO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_params(params: hk.Params):\n",
        "  num_params = 0\n",
        "  for p in jax.tree_leaves(params): \n",
        "    # print(p.shape)\n",
        "    num_params = num_params + jnp.prod(p.shape)\n",
        "  return num_params\n",
        "print('Total number of parameters %d' % get_num_params(params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob9HedFyMa8I",
        "colab_type": "text"
      },
      "source": [
        "### Display input images and shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEZB8A4rP3-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (batch['image'].shape)\n",
        "print (batch['label'].shape)\n",
        "gallery(batch['image'], batch['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZkweB3XMpN-",
        "colab_type": "text"
      },
      "source": [
        "### Run training loop and evaluation; full training gives accuracy ~89.1%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH3mhqv_VUGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_every = train_eval_every\n",
        "log_every = train_log_every\n",
        "\n",
        "for step_num in range(int(num_train_steps)):\n",
        "  # Take a training step.\n",
        "  params, state, opt_state, train_loss = (\n",
        "      train_step(params, state, opt_state, next(train_dataset)))\n",
        "\n",
        "  # We run evaluation during training to see the progress.\n",
        "  if eval_every > 0 and step_num % eval_every == 0:\n",
        "    eval_acc = evaluate(eval_split, params, state)\n",
        "    print('[Eval acc %s/%s] %s'%(step_num, int(num_train_steps), eval_acc))\n",
        "\n",
        "  # Log progress at fixed intervals.\n",
        "  if step_num % log_every == 0:\n",
        "    print('[Train loss %s/%s] %s'%(step_num, int(num_train_steps), train_loss))\n",
        "\n",
        "# Once training has finished we run eval one more time to get final results.\n",
        "eval_acc = evaluate(eval_split, params, state)\n",
        "print('[Eval acc FINAL]: %s'%(eval_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
