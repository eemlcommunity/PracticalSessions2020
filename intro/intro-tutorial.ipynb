{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"intro-tutorial.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"X8yw_hrKYuL6","colab_type":"text"},"source":["# Introduction to Colab, Jax, haiku\n","Authors: Viorica Patraucean, David Szepesvari\n","\n","Contact: vpatrauc@gmail.com\n","\n","Thanks to Carl Doersch and Stanislaw Jastrzebski for proofreading and advice."]},{"cell_type":"markdown","metadata":{"id":"lsr0IKnOYEUi","colab_type":"text"},"source":["## What is Colab?\n","\n","[Colaboratory](https://colab.sandbox.google.com/notebooks/welcome.ipynb) is a [Jupyter](http://jupyter.org/) notebook environment that requires no setup to use. It allows you to create and share documents that contain\n","\n","* Live, runnable code\n","* Visualizations\n","* Explanatory text\n","\n","It's also a great tool for prototyping and quick development. Let's give it a try. "]},{"cell_type":"markdown","metadata":{"id":"wZjpiZSDYJ1R","colab_type":"text"},"source":["Run the following so-called *(Code) Cell* by moving the cursor into it, and either\n","\n","* Pressing the \"play\" icon on the left of the cell, or\n","* Hitting **`Shift + Enter`**."]},{"cell_type":"code","metadata":{"id":"VIRY1OxWYNED","colab_type":"code","colab":{}},"source":["print('Hello, onlineEEML2020!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okXieYk8YSZ5","colab_type":"text"},"source":["You should see the `Hello, onlineEEML2020!` printed under the code."]},{"cell_type":"markdown","metadata":{"id":"JFIWmbiyYb_A","colab_type":"text"},"source":["The code is executed on a virtual machine dedicated to your account, with the results sent back to your browser. This has some positive and negative consequences."]},{"cell_type":"markdown","metadata":{"id":"xXzTe0-8Yk12","colab_type":"text"},"source":["### Using a GPU\n","\n","You can connect to a virtual machine with a GPU. To select the hardware you want to use, follow either\n","\n","* **Edit > Notebook settings**, or\n","* **Runtime > Change runtime type**\n","\n","and choose an accelerator."]},{"cell_type":"markdown","metadata":{"id":"USfPlwlyY0oK","colab_type":"text"},"source":["### Losing Connection\n","\n","You may lose connection to your virtual machine. The two most common causes are\n","\n","* Virtual machines are recycled when idle for a while, and have a maximum lifetime enforced by the system.\n","* Long-running background computations, particularly on GPUs, may be stopped.\n","\n","**If you lose connection**, the state of your notebook will also be lost. You will need to **rerun all cells** up to the one you are currently working on. To do so\n","\n","1. Select (place the cursor into) the cell you are working on. \n","2. Follow **Runtime > Run before**."]},{"cell_type":"markdown","metadata":{"id":"oWgJEOspaQA_","colab_type":"text"},"source":["### Pretty Printing by colab\n","1) If the **last operation** of a given cell returns a value, it will be pretty printed by colab.\n"]},{"cell_type":"code","metadata":{"id":"ttk-hcG2aT7M","colab_type":"code","colab":{}},"source":["6 * 7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8hMw-vRaXP1","colab_type":"code","colab":{}},"source":["my_dict = {'one': 1, 'some set': {4, 2, 2}, 'a regular list': range(5)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CNs8mVyYaa-n","colab_type":"text"},"source":["There is no output from the second cell, as assignment does not return anything."]},{"cell_type":"markdown","metadata":{"id":"Yg4BV2bXaeqT","colab_type":"text"},"source":["2) You can explicitly **print** anything before the last operation, or **supress** the output of the last operation by adding a semicolon."]},{"cell_type":"code","metadata":{"id":"mn0aHa_pajOT","colab_type":"code","colab":{}},"source":["print(my_dict)\n","my_dict['one'] * 10 + 1;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFVpqvB-ZTot","colab_type":"text"},"source":["### Scoping and Execution Model"]},{"cell_type":"markdown","metadata":{"id":"AdeQn9ExapJM","colab_type":"text"},"source":["Notice that in the previous code cell we worked with `my_dict`, while it was defined in an even earlier cell.\n","\n","1) In colabs, variables defined at cell root have **global** scope.\n","\n","Modify `my_dict`:"]},{"cell_type":"code","metadata":{"id":"wx4_0k3Ka6KR","colab_type":"code","colab":{}},"source":["my_dict['I\\'ve been changed!'] = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XsXP0Spya_NG","colab_type":"text"},"source":["2) Cells can be **run** in any **arbitrary order**, and global state is maintained between them.\n","\n","Try re-running the cell where we printed `my_dict`. You should see now  see the additional item `\"I've been changed!\": True`."]},{"cell_type":"markdown","metadata":{"id":"vFofwmqAZP6s","colab_type":"text"},"source":["3) Unintentionally reusing a global variable can lead to bugs. If all else fails, you can uncomment and run the following line to **clear all global variables** and run again all the cells."]},{"cell_type":"code","metadata":{"id":"ugbgF-kCZoE-","colab_type":"code","colab":{}},"source":["# %reset -f"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r8n7dyqqZ5sf","colab_type":"text"},"source":["### Autocomplete / Documentation\n","\n","* Press *`<TAB>`* after typing a prefix will show the available variables / commands.\n","* Press *`<TAB>`* on a function parameter list will show the function documentation.\n","\n","Note: this only works for variables that are already been defined (not while you are writing your code)."]},{"cell_type":"markdown","metadata":{"id":"TRWQOmFKbSNW","colab_type":"text"},"source":["### Setup and Imports\n","\n","Python packages can and need to be imported into your colab notebook, the same way you would import them in a python script. For example, to use `numpy`, you would do"]},{"cell_type":"code","metadata":{"id":"oLiqqd44bg3B","colab_type":"code","colab":{}},"source":["# import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9_Kj04gbXsf","colab_type":"text"},"source":["While many packages can just be imported, some (e.g. `haiku`, a neural network library from DeepMind) may not be prepackaged in the runtime. With Colab, you can install any python package from `pip` for the duration of your connection."]},{"cell_type":"code","metadata":{"id":"SAmMS5h8bn_1","colab_type":"code","colab":{}},"source":["# we will use haiku on top of jax \n","# !pip install -q dm-haiku\n","# import haiku as hk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXYkeehcb8kg","colab_type":"text"},"source":["### Forms\n","\n","With colab it is easy to take input from the user in code cells through so called forms. A simplest example is shown below."]},{"cell_type":"code","metadata":{"id":"IAZhaY6NcXlc","colab_type":"code","colab":{}},"source":["#@title This text shows up as a title.\n","\n","a = 2  #@param {type: 'integer'}\n","b = 3  #@param\n","\n","print('a+b =', str(a+b))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDlY63E-ccL7","colab_type":"text"},"source":["You can change parameters on the right hand side, then rerun the cell to use these values. **Try setting the value of a=5 and rerun the cell above.**\n","\n","In order to expose a variable as parameter you just add `#@param` after it. There are various knds of params, if you're interested you can read more about this on the official starting colab."]},{"cell_type":"markdown","metadata":{"id":"K_RVl7E8cgut","colab_type":"text"},"source":["Cells with forms allow you to toggle whether\n","\n","* the code,\n","* the form,\n","* or both\n","\n","are visible.\n","\n","**Try switching between these 3 options for the above cell.** This is how you do this:\n","\n","1. Click anywhere over the area of the cell with the form to highlight it.\n","2. Click on the \"three vertically arranged dots\" icong in the top right of the cell.\n","3. Go to \"Form >\", select your desired action."]},{"cell_type":"markdown","metadata":{"id":"rjPF8rKiize7","colab_type":"text"},"source":["## JAX\n","[JAX](https://jax.readthedocs.io/en/latest/jax.html) allows NumPy-like code to execute on CPU, or accelerators like GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n","\n","- Jax automatically differentiates python code and NumPy code (with [Autograd](https://github.com/hips/autograd))\n","- uses [XLA](https://www.tensorflow.org/xla) to compile and run NumPy code efficiently on accelerators"]},{"cell_type":"markdown","metadata":{"id":"NDmbQoL5mYmA","colab_type":"text"},"source":["### JAX and random number generators (WIP)\n","Unlike many ML frameworks, JAX does not hide the pseudo-random number generator state. You need to generate explicitely a random key, and pass it to the operations that work with random numbers (e.g. initialising a model, dropout etc). A call to a random function with the same key does not change the state of the generator. This has to be done explicitely with `split()` or `next_rng_key()` in `haiku`."]},{"cell_type":"code","metadata":{"id":"0j5kLgolmlCl","colab_type":"code","colab":{}},"source":["import numpy as np\n","import jax.numpy as jnp\n","from jax import random\n","key = random.PRNGKey(0)\n","x1 = random.normal(key, (3,))\n","print(x1)\n","x2 = random.normal(key, (3,))\n","print(x2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZNs5VAdfmpG","colab_type":"code","colab":{}},"source":["# Let's split the key to be able to generate different random values\n","key, new_key = random.split(key)\n","x1 = random.normal(key, (3,))\n","print (x1)\n","x2 = random.normal(new_key, (3,))\n","print (x2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hiqCH1Nkvdv","colab_type":"text"},"source":["### JAX program transformations with examples \n","* `jit` (just-in-time compilation) -- speeds up your code by running all the ops inside the jit-ed function as a *fused* op; it compiles the function when it's called the first time and uses the compiled (optimised) version from the second call onwards.\n","* `grad` -- returns derivatives of function with respect to the model weights passed as parameters\n","* `vmap` -- automatic batching; returns a new function that can apply the original (per-sample) function to a batch.\n","\n"]},{"cell_type":"code","metadata":{"id":"qiWR4CPjlc6T","colab_type":"code","colab":{}},"source":["from jax import grad, jit\n","# Let's use jit to speed up a function\n","def selu(x, alpha=1.67, lmbda=1.05):\n","  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n","\n","# execute the function without jit\n","x = random.normal(key, (1000000,))\n","%timeit selu(x).block_until_ready()   # block_until_ready is needed as jax, by default, runs operations asyncronously\n","\n","# Execute the function with jit and compare timing with above -- it should be much faster\n","selu_jit = jit(selu)\n","%timeit selu_jit(x).block_until_ready()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rP70-aR3oouX","colab_type":"code","colab":{}},"source":["# Let's use grad to compute gradient of a simple function\n","def simple_fun(x):\n","  return jnp.sin(x) / x\n","\n","# Get the gradient of simple_fun with respect to x\n","grad_simple_fun = grad(simple_fun)\n","\n","# We can also get higher order derivatives, e.g. Hessian\n","grad_grad_simple_fun = grad(grad(simple_fun))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FH8uTx364T0m","colab_type":"code","colab":{}},"source":["# Let's plot the result\n","import matplotlib.pyplot as plt\n","x_range = jnp.arange(-8, 8, .1)\n","plt.plot(x_range, simple_fun(x_range), 'b')\n","plt.plot(x_range, [grad_simple_fun(xi) for xi in x_range], 'r')\n","plt.plot(x_range, [grad_grad_simple_fun(xi) for xi in x_range], '--g')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"foOt9Hq2_dqI","colab_type":"code","colab":{}},"source":["from jax import vmap\n","# Let's see how vmap can be used to vectorize computations efficiently\n","# In the example above, we can use vmap instead of loop to compute gradients\n","\n","grad_vect_simple_fun = vmap(grad_simple_fun)(x_range)\n","\n","# plot again and check that the gradients are identical \n","plt.plot(x_range, simple_fun(x_range), 'b')\n","plt.plot(x_range, [grad_simple_fun(xi) for xi in x_range], 'r')\n","plt.plot(x_range, grad_vect_simple_fun, 'oc', mfc='none')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dibhhYsjphE3","colab_type":"code","colab":{}},"source":["# Let's time them!\n","\n","# naive batching\n","def naively_batched(x):\n","  return jnp.stack([grad_simple_fun(xi) for xi in x])\n","\n","# manual batching with jit\n","@jit\n","def manual_batched(x):\n","  return jnp.stack([grad_simple_fun(xi) for xi in x])\n","\n","# Batching using vmap and jit\n","@jit\n","def vmap_batched(x):\n","  return vmap(grad_simple_fun)(x)\n","\n","print ('Naively batched')\n","%timeit naively_batched(x_range).block_until_ready()\n","print ('jit batched')\n","%timeit manual_batched(x_range).block_until_ready()\n","print ('With jit vmap')\n","%timeit vmap_batched(x_range).block_until_ready()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0lHGcAwdfQq","colab_type":"text"},"source":["### Read the doc for [common gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) in JAX!"]},{"cell_type":"markdown","metadata":{"id":"x7ViyN6Q8kOa","colab_type":"text"},"source":["## Haiku -- object-oriented neural network library on top of JAX\n","Notable functions / entities\n","* `hk.Module` base class: implement your own modules by deriving from it\n","* `hk.transform`: convert non-pure (objects) functions into pure functions; returns a pair of pure functions `init` and `apply`.\n","* `hk.next_rng_key()`: returns a unique random key\n"]},{"cell_type":"markdown","metadata":{"id":"3h4MVQzkck_R","colab_type":"text"},"source":["### Example: Train MLP classifier on MNIST"]},{"cell_type":"code","metadata":{"id":"NG3krUq1WK_A","colab_type":"code","colab":{}},"source":["import contextlib\n","from typing import Any, Mapping, Generator, Tuple \n","\n","# we will use haiku on top of jax \n","!pip install -q dm-haiku\n","import haiku as hk\n","\n","import jax\n","from jax.experimental import optix  # package for optimisers\n","import jax.numpy as jnp\n","import numpy as np\n","import enum\n","\n","# Dataset library\n","import tensorflow.compat.v2 as tf\n","import tensorflow_datasets as tfds\n","\n","# Plotting library.\n","from matplotlib import pyplot as plt\n","import pylab as pl\n","from IPython import display\n","\n","# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","# define some useful types\n","OptState = Any\n","Batch = Mapping[str, np.ndarray]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6r0EfeAqIZkS","colab_type":"text"},"source":["### Define the dataset: MNIST"]},{"cell_type":"code","metadata":{"id":"ZDP4M0aVWpdB","colab_type":"code","colab":{}},"source":["# We use TF datasets; JAX does not support data loading or preprocessing.\n","NUM_CLASSES = 10  # MNIST has 10 classes, corresponding to the different digits.\n","def load_dataset(\n","    split: str,\n","    *,\n","    is_training: bool,\n","    batch_size: int,\n",") -> Generator[Batch, None, None]:\n","  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n","  ds = tfds.load('mnist:3.*.*', split=split).cache().repeat()\n","  if is_training:\n","    ds = ds.shuffle(10 * batch_size, seed=0)\n","  ds = ds.batch(batch_size)\n","  return tfds.as_numpy(ds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2OYWEKKgVEJ","colab_type":"code","colab":{}},"source":["# Function to display images\n","MAX_IMAGES = 10\n","def gallery(images, label, title='Input images'):  \n","  class_dict = [u'zero', u'one', u'two', u'three', u'four', u'five', u'six', u'seven', u'eight', u'nine']\n","  num_frames, h, w, num_channels = images.shape\n","  num_frames = min(num_frames, MAX_IMAGES)\n","  ff, axes = plt.subplots(1, num_frames,\n","                          figsize=(30, 30),\n","                          subplot_kw={'xticks': [], 'yticks': []})\n","  if images.min() < 0:\n","    images = (images + 1.) / 2.\n","  for i in range(0, num_frames):\n","    if num_channels == 3:\n","      axes[i].imshow(np.squeeze(images[i]))\n","    else:\n","      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n","    axes[i].set_title(class_dict[label[i]], fontsize=28)\n","    plt.setp(axes[i].get_xticklabels(), visible=False)\n","    plt.setp(axes[i].get_yticklabels(), visible=False)\n","  ff.subplots_adjust(wspace=0.1)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMXfrKnof2L3","colab_type":"code","colab":{}},"source":["# Make datasets for train and test\n","train_dataset = load_dataset('train', is_training=True, batch_size=1000)\n","train_eval_dataset = load_dataset('train', is_training=False, batch_size=10000)\n","test_eval_dataset = load_dataset('test', is_training=False, batch_size=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00m3RHu0BTir","colab_type":"text"},"source":["### Define classifier: a simple MLP"]},{"cell_type":"code","metadata":{"id":"7j58v6LhWjrQ","colab_type":"code","colab":{}},"source":["def net_fn(batch: Batch) -> jnp.ndarray:\n","  \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n","  # The images are in [0, 255], uint8; we need to convert to float and normalise \n","  x = batch['image'].astype(jnp.float32) / 255.\n","  # We use hk.Sequential to chain the modules in the network\n","  mlp = hk.Sequential([\n","  # The input images are 28x28, so we first flatten them to apply linear (fully-connected) layers                     \n","      hk.Flatten(),  \n","      hk.Linear(300), jax.nn.relu,\n","      hk.Linear(100), jax.nn.relu,\n","      hk.Linear(10),\n","  ])\n","  return mlp(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH5-IvPkMftZ","colab_type":"text"},"source":["### Retrieve pure functions for our model (`init`, `apply`) using `hk.transform`\n","\n"]},{"cell_type":"code","metadata":{"id":"a9YzGJ_4WuE5","colab_type":"code","colab":{}},"source":["# Since we don't store additional state statistics, e.g. needed in batch norm,\n","# we use `hk.transform`. When we use batch_norm, we will use `hk.transform_with_state`\n","net = hk.transform(net_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qfm8NSdQOR1n","colab_type":"text"},"source":["### Define the optimiser"]},{"cell_type":"code","metadata":{"id":"xxXMm5OzOPCO","colab_type":"code","colab":{}},"source":["# We use Adam optimizer here. Others are possible, e.g. sgd with momentum.\n","lr = 1e-3\n","opt = optix.adam(lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GA81esw-OoBK","colab_type":"text"},"source":["### Define the optimisation objective"]},{"cell_type":"code","metadata":{"id":"OA1FdFSwWwtb","colab_type":"code","colab":{}},"source":["# Training loss: cross-entropy plus regularization weight decay\n","def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n","  \"\"\"Compute the loss of the network, including L2 for regularization.\"\"\"\n","  \n","  # Get network predictions\n","  logits = net.apply(params, batch)\n","\n","  # Generate one_hot labels from index classes\n","  labels = jax.nn.one_hot(batch['label'], NUM_CLASSES)\n","\n","  # Compute mean softmax cross entropy over the batch\n","  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n","  softmax_xent /= labels.shape[0]\n","\n","  # Compute the weight decay loss by penalising the norm of parameters\n","  l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n","  \n","  return softmax_xent + 1e-4 * l2_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KA_CZhFHQDDN","colab_type":"text"},"source":["### Evaluation metric"]},{"cell_type":"code","metadata":{"id":"dlEMA3RCXBU_","colab_type":"code","colab":{}},"source":["# Classification accuracy\n","@jax.jit\n","def accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n","  # Get network predictions\n","  predictions = net.apply(params, batch)\n","  # Return accuracy = how many predictions match the ground truth\n","  return jnp.mean(jnp.argmax(predictions, axis=-1) == batch['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SGCtelDQfOv","colab_type":"text"},"source":["### Define training step (parameters update)"]},{"cell_type":"code","metadata":{"id":"PsjUbkMWQlt-","colab_type":"code","colab":{}},"source":["@jax.jit\n","def update(\n","    params: hk.Params,\n","    opt_state: OptState,\n","    batch: Batch,\n",") -> Tuple[hk.Params, OptState]:\n","  \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n","  # Use jax transformation `grad` to compute gradients; \n","  # it expects the prameters of the model and the input batch\n","  grads = jax.grad(loss)(params, batch)\n","\n","  # Compute parameters updates based on gradients and optimiser state\n","  updates, opt_state = opt.update(grads, opt_state)\n","\n","  # Apply updates to parameters\n","  new_params = optix.apply_updates(params, updates)\n","  return new_params, opt_state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RubrTm2_WOiP","colab_type":"text"},"source":["### Initialise the model and the optimiser"]},{"cell_type":"code","metadata":{"id":"2a8lZDaAmH3B","colab_type":"code","colab":{}},"source":["# Initialize model and optimiser; note that a sample input is needed to compute\n","# shapes of parameters\n","\n","# Draw a data batch\n","batch = next(train_dataset)\n","# Initialize model\n","params = net.init(jax.random.PRNGKey(42), batch)\n","#Initialize optimiser\n","opt_state = opt.init(params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h2yunmBjWyPQ","colab_type":"text"},"source":["### Visualise data and parameter shapes"]},{"cell_type":"code","metadata":{"id":"SSmLtlnUgBeS","colab_type":"code","colab":{}},"source":["# Display shapes and images\n","print(batch['image'].shape)\n","print(batch['label'].shape)\n","gallery(batch['image'], batch['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"euhMKRF_W5_D","colab_type":"code","colab":{}},"source":["# Let's see how many parameters in our network and their shapes\n","def get_num_params(params: hk.Params):\n","  num_params = 0\n","  for p in jax.tree_leaves(params): \n","    print(p.shape)\n","    num_params = num_params + jnp.prod(p.shape)\n","  return num_params\n","print('Total number of parameters %d' % get_num_params(params))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uq1OddpfX-6y","colab_type":"text"},"source":["### Accuracy of the untrained model (should be ~10%)"]},{"cell_type":"code","metadata":{"id":"dHzblZx6X9jH","colab_type":"code","colab":{}},"source":["# Run accuracy on the test dataset\n","test_accuracy = accuracy(params, next(test_eval_dataset))\n","print('Test accuracy %f '% test_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wIiw1dPDZEuN","colab_type":"code","colab":{}},"source":["# Let's visualise some network predictions before training; if some are correct,\n","# they are correct by chance.\n","predictions = net.apply(params, batch)\n","pred_labels = jnp.argmax(predictions, axis=-1)\n","gallery(batch['image'], pred_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjeMJxKYaeN-","colab_type":"text"},"source":["### Run one training step"]},{"cell_type":"code","metadata":{"id":"ejfhiuoiaiZ6","colab_type":"code","colab":{}},"source":["# First, let's do one step and check if the updates lead to decrease in error\n","loss_before_train = loss(params, batch) \n","print('Loss before train %f' % loss_before_train)\n","params, opt_state = update(params, opt_state, batch)\n","new_loss = loss(params, next(train_dataset))\n","new_loss_same_batch = loss(params, batch)\n","print('Loss after one step of training, same batch %f, different batch %f' % (new_loss_same_batch, new_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9QFlfNTW-GZ","colab_type":"text"},"source":["### Run training steps in a loop. We also run evaluation periodically."]},{"cell_type":"code","metadata":{"id":"fAR5joBwV5cT","colab_type":"code","colab":{}},"source":["# Train/eval loop.\n","for step in range(5001):\n","  if step % 1000 == 0:\n","    # Periodically evaluate classification accuracy on train & test sets.\n","    train_accuracy = accuracy(params, next(train_eval_dataset))\n","    test_accuracy = accuracy(params, next(test_eval_dataset))\n","    train_accuracy, test_accuracy = jax.device_get(\n","        (train_accuracy, test_accuracy))\n","    print('Step %d Train / Test accuracy: %f / %f' % (step, train_accuracy, test_accuracy))\n","\n","  # Do SGD on a batch of training examples.\n","  params, opt_state = update(params, opt_state, next(train_dataset))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNYLWtvmao4i","colab_type":"text"},"source":["### Visualise network predictions after training; most of the predictions should be correct."]},{"cell_type":"code","metadata":{"id":"mGb8B6n6au9L","colab_type":"code","colab":{}},"source":["# Get predictions for the same batch\n","predictions = net.apply(params, batch)\n","pred_labels = jnp.argmax(predictions, axis=-1)\n","gallery(batch['image'], pred_labels)"],"execution_count":null,"outputs":[]}]}